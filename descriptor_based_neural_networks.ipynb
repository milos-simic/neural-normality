{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Set-up-the-Environment\" data-toc-modified-id=\"Set-up-the-Environment-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Set up the Environment</a></span></li><li><span><a href=\"#Prepare-the-Data\" data-toc-modified-id=\"Prepare-the-Data-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Prepare the Data</a></span><ul class=\"toc-item\"><li><span><a href=\"#Load-the-Data\" data-toc-modified-id=\"Load-the-Data-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Load the Data</a></span></li><li><span><a href=\"#Split-$\\mathcal{A}$-into-Cross-validation-and-Test--Subsets-($\\mathcal{A}_{cv}$-and-$\\mathcal{A}_{test}$)\" data-toc-modified-id=\"Split-$\\mathcal{A}$-into-Cross-validation-and-Test--Subsets-($\\mathcal{A}_{cv}$-and-$\\mathcal{A}_{test}$)-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Split $\\mathcal{A}$ into Cross-validation and Test  Subsets ($\\mathcal{A}_{cv}$ and $\\mathcal{A}_{test}$)</a></span></li><li><span><a href=\"#Split-the-Labels-and-the-Samples-in-Other-Datasets\" data-toc-modified-id=\"Split-the-Labels-and-the-Samples-in-Other-Datasets-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>Split the Labels and the Samples in Other Datasets</a></span></li></ul></li><li><span><a href=\"#Construct-the-Descriptor-based-Feedforward-Neural-Network-Classifier-(DBNN)\" data-toc-modified-id=\"Construct-the-Descriptor-based-Feedforward-Neural-Network-Classifier-(DBNN)-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Construct the Descriptor-based Feedforward Neural Network Classifier (DBNN)</a></span><ul class=\"toc-item\"><li><span><a href=\"#Prepare-the-Descriptor-Builder\" data-toc-modified-id=\"Prepare-the-Descriptor-Builder-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Prepare the Descriptor Builder</a></span></li><li><span><a href=\"#Cross-validate\" data-toc-modified-id=\"Cross-validate-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Cross-validate</a></span><ul class=\"toc-item\"><li><span><a href=\"#Show-and-Store-the-CV-Results-and-Reports\" data-toc-modified-id=\"Show-and-Store-the-CV-Results-and-Reports-3.2.1\"><span class=\"toc-item-num\">3.2.1&nbsp;&nbsp;</span>Show and Store the CV Results and Reports</a></span></li></ul></li></ul></li><li><span><a href=\"#Check-if-the-Descriptors-are-Well-formed\" data-toc-modified-id=\"Check-if-the-Descriptors-are-Well-formed-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Check if the Descriptors are Well-formed</a></span></li><li><span><a href=\"#Check-Calibration\" data-toc-modified-id=\"Check-Calibration-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Check Calibration</a></span></li><li><span><a href=\"#Evaluate-Classification-Performance\" data-toc-modified-id=\"Evaluate-Classification-Performance-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Evaluate Classification Performance</a></span><ul class=\"toc-item\"><li><span><a href=\"#Performance-on-Set-$\\mathcal{A}_{test}$\" data-toc-modified-id=\"Performance-on-Set-$\\mathcal{A}_{test}$-6.1\"><span class=\"toc-item-num\">6.1&nbsp;&nbsp;</span>Performance on Set $\\mathcal{A}_{test}$</a></span></li><li><span><a href=\"#Performance-on-Set-$\\mathcal{B}$\" data-toc-modified-id=\"Performance-on-Set-$\\mathcal{B}$-6.2\"><span class=\"toc-item-num\">6.2&nbsp;&nbsp;</span>Performance on Set $\\mathcal{B}$</a></span><ul class=\"toc-item\"><li><span><a href=\"#Compare-Performance-with-that-on-$\\mathcal{A}_{test}$\" data-toc-modified-id=\"Compare-Performance-with-that-on-$\\mathcal{A}_{test}$-6.2.1\"><span class=\"toc-item-num\">6.2.1&nbsp;&nbsp;</span>Compare Performance with that on $\\mathcal{A}_{test}$</a></span></li></ul></li><li><span><a href=\"#Performance-on-Set-$\\mathcal{C}$\" data-toc-modified-id=\"Performance-on-Set-$\\mathcal{C}$-6.3\"><span class=\"toc-item-num\">6.3&nbsp;&nbsp;</span>Performance on Set $\\mathcal{C}$</a></span></li></ul></li><li><span><a href=\"#Compare-DBNN-with-the-Selected-Standard-Statistical-Tests-of-Normality\" data-toc-modified-id=\"Compare-DBNN-with-the-Selected-Standard-Statistical-Tests-of-Normality-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Compare DBNN with the Selected Standard Statistical Tests of Normality</a></span><ul class=\"toc-item\"><li><span><a href=\"#Compare-the-TNR-Scores-Using-the-Set-$\\mathcal{C}$\" data-toc-modified-id=\"Compare-the-TNR-Scores-Using-the-Set-$\\mathcal{C}$-7.1\"><span class=\"toc-item-num\">7.1&nbsp;&nbsp;</span>Compare the TNR Scores Using the Set $\\mathcal{C}$</a></span></li><li><span><a href=\"#Compare-Performance-Using-Set-$\\mathcal{D}$\" data-toc-modified-id=\"Compare-Performance-Using-Set-$\\mathcal{D}$-7.2\"><span class=\"toc-item-num\">7.2&nbsp;&nbsp;</span>Compare Performance Using Set $\\mathcal{D}$</a></span><ul class=\"toc-item\"><li><span><a href=\"#Conduct-ROC-Analysis\" data-toc-modified-id=\"Conduct-ROC-Analysis-7.2.1\"><span class=\"toc-item-num\">7.2.1&nbsp;&nbsp;</span>Conduct ROC Analysis</a></span></li></ul></li><li><span><a href=\"#Comparison-on-Real-World-Data\" data-toc-modified-id=\"Comparison-on-Real-World-Data-7.3\"><span class=\"toc-item-num\">7.3&nbsp;&nbsp;</span>Comparison on Real-World Data</a></span><ul class=\"toc-item\"><li><span><a href=\"#$\\mathcal{R}_{height}$\" data-toc-modified-id=\"$\\mathcal{R}_{height}$-7.3.1\"><span class=\"toc-item-num\">7.3.1&nbsp;&nbsp;</span>$\\mathcal{R}_{height}$</a></span></li><li><span><a href=\"#$\\mathcal{R}_{earthquake}$\" data-toc-modified-id=\"$\\mathcal{R}_{earthquake}$-7.3.2\"><span class=\"toc-item-num\">7.3.2&nbsp;&nbsp;</span>$\\mathcal{R}_{earthquake}$</a></span></li></ul></li></ul></li><li><span><a href=\"#Runtime-Analysis\" data-toc-modified-id=\"Runtime-Analysis-8\"><span class=\"toc-item-num\">8&nbsp;&nbsp;</span>Runtime Analysis</a></span></li><li><span><a href=\"#Evaluation-on-Larger-Samples\" data-toc-modified-id=\"Evaluation-on-Larger-Samples-9\"><span class=\"toc-item-num\">9&nbsp;&nbsp;</span>Evaluation on Larger Samples</a></span></li><li><span><a href=\"#Save\" data-toc-modified-id=\"Save-10\"><span class=\"toc-item-num\">10&nbsp;&nbsp;</span>Save</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Descriptor-Based Neural Networks for Normality Testing <a class=\"tocSkip\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up the Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-02T14:49:33.312115Z",
     "start_time": "2020-11-02T14:49:33.300479Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Import everything that's needed to run the notebook\n",
    "import os\n",
    "import pickle\n",
    "import pathlib\n",
    "import datetime\n",
    "import random\n",
    "\n",
    "from IPython.display import display, Markdown, Latex\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "import sklearn.metrics\n",
    "import sklearn.preprocessing\n",
    "import scipy.stats\n",
    "import matplotlib.pyplot as plt\n",
    "import boruta\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "import util\n",
    "from ipynb.fs.defs.sbnn import SBNNPreprocessor\n",
    "from sklearn.model_selection import learning_curve\n",
    "\n",
    "plt.rc('xtick',labelsize=13)\n",
    "plt.rc('ytick',labelsize=13)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-02T13:03:21.048393Z",
     "start_time": "2020-11-02T13:03:21.044248Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define the path to the configuration dictionary\n",
    "config_path = 'configuration.p'\n",
    "\n",
    "# Load the configuration dictionary\n",
    "with open(config_path, 'rb') as f:\n",
    "    configuration = pickle.load(f)\n",
    "    \n",
    "# Get the paths to the relevant directories \n",
    "data_directory_path = configuration['data']['directory_path']\n",
    "classifiers_directory_path = configuration['classifiers']['directory_path']\n",
    "\n",
    "# Get the parameters of the experiment\n",
    "cv_folds = configuration['experiment']['number_of_cv_folds']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the datasets using the function `load_from_file` from `util`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-02T13:04:37.251345Z",
     "start_time": "2020-11-02T13:04:25.564100Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define the dictionary to store the actual datasets, indexed by their names\n",
    "datasets = {}\n",
    "\n",
    "# Load the datasets\n",
    "for set_name in ['A', 'B', 'C-G1', 'C-G2', 'C-G3', 'C-G4', 'D', 'R_height', 'R_earthquake']:\n",
    "    set_path = configuration['data']['datasets'][set_name]['path']\n",
    "    print('Loading {} from {}'.format(set_name, set_path))\n",
    "    datasets[set_name] = util.load_from_file(set_path)\n",
    "    print('Done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show a normal sample from the dataset $\\mathcal{A}$. and a non-normal from the group $G_1$ of the dataset $\\mathcal{C}$. The normal samples are labeled with $1$, whereas the label of each non-normal one is $0$. The normal samples constitute the \"positive\" and the non-normal the \"negative\" class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-02T13:04:47.206437Z",
     "start_time": "2020-11-02T13:04:47.199215Z"
    }
   },
   "outputs": [],
   "source": [
    "print('A normal sample (ending in 1):\\n', datasets['A'][0])\n",
    "print('A non-normal sample (ending in 0):\\n', datasets['C-G1'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split $\\mathcal{A}$ into Cross-validation and Test  Subsets ($\\mathcal{A}_{cv}$ and $\\mathcal{A}_{test}$) \n",
    "\n",
    "Split $\\mathcal{A}$ into the subsets for cross-validation ($\\mathcal{A}_{cv})$ and testing ($\\mathcal{A}_{test}$).\n",
    "\n",
    "Use 70% of the set to cross-validate and 30% for subsequent testing and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-02T13:05:03.176879Z",
     "start_time": "2020-11-02T13:05:03.120924Z"
    }
   },
   "outputs": [],
   "source": [
    "# Extract the labels from the set, leaving only samples in it\n",
    "labels_A = [labeled_sample.pop() for labeled_sample in datasets['A']]\n",
    "samples_A = datasets['A']\n",
    "\n",
    "# There is no need to store the sama data twice, in datasets['A'] and in (samples_A, labels_A)\n",
    "del datasets['A']\n",
    "\n",
    "# Define the stratification labels as the combination of actual labels and sample sizes\n",
    "stratification_labels = [str(label) + str(len(sample)) for (label, sample) in zip(labels_A, samples_A)]\n",
    "\n",
    "# Set the relative size of the CV subset\n",
    "train_size = 0.7\n",
    "\n",
    "# Split the data into CV and test subsets\n",
    "random_state=None\n",
    "set_A_cv, set_A_test, y_cv, y_test = train_test_split(samples_A, labels_A, stratify=stratification_labels, \n",
    "                                              train_size=train_size, random_state=random_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the Labels and the Samples in Other Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-02T13:05:37.838171Z",
     "start_time": "2020-11-02T13:05:37.749979Z"
    }
   },
   "outputs": [],
   "source": [
    "for set_name in datasets:\n",
    "    labels = [sample.pop() for sample in datasets[set_name]]\n",
    "    samples = datasets[set_name]\n",
    "    \n",
    "    datasets[set_name] = {'samples' : samples, 'labels' : labels}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct the Descriptor-based Feedforward Neural Network Classifier (DBNN)\n",
    "\n",
    "First, register DBNN as a classifer in the configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-02T13:05:44.329769Z",
     "start_time": "2020-11-02T13:05:44.325768Z"
    }
   },
   "outputs": [],
   "source": [
    "dbnn_metadata = {\n",
    "    'name' : 'dbnn',\n",
    "    'path' :os.path.join(configuration['classifiers']['directory_path'], 'dbnn.p'),\n",
    "}\n",
    "\n",
    "configuration['classifiers']['classifiers']['dbnn'] = dbnn_metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a storage to hold classifier, the results and reports (figures, dataframes and $\\LaTeX$ tables)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-02T13:05:45.968840Z",
     "start_time": "2020-11-02T13:05:45.961794Z"
    }
   },
   "outputs": [],
   "source": [
    "dbnn_storage = {\n",
    "    'classifier' : None,\n",
    "    'classifier_based_test' : None,\n",
    "    'test_based_classifer' : None,\n",
    "    'results' : {\n",
    "        'cv' : {},\n",
    "        'evaluation' : {},\n",
    "        'comparison' : {}\n",
    "    },\n",
    "    'reports' : {\n",
    "        'cv' : {},\n",
    "        'evaluation' : {},\n",
    "        'comparison' : {}\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the Descriptor Builder\n",
    "\n",
    "Since the neural networks we devise in this notebook operate with descriptors and not the raw samples, define the class `DescriptorBuilder` to perform that preprocessing step and to be plugged into `sklearn` pipeline.\n",
    "\n",
    "To do so, define two separate functions: a routine to find the sample's empirical quantiles and a function to determine the descriptor of a given sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-02T13:05:48.793189Z",
     "start_time": "2020-11-02T13:05:48.787521Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define the function that finds the lowest empirical quantile \n",
    "# that is greater than 100q% of the sample (0 < q <= 1).\n",
    "# The sample has to be sorted in the ascending order in order for this function to work.\n",
    "\n",
    "def get_quantile(sample, q):\n",
    "    n = len(sample)\n",
    "    i = 0\n",
    "    s = 1.0 / n\n",
    "    \n",
    "    while s < q and i < n:\n",
    "        s = s + 1.0 / n\n",
    "        i = i + 1\n",
    "\n",
    "    if i == n:\n",
    "        i = n - 1\n",
    "\n",
    "    return sample[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-02T13:05:49.749892Z",
     "start_time": "2020-11-02T13:05:49.741480Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define the function to construct a sample's descriptor.\n",
    "def get_descriptor(sample, q, sorted=True):\n",
    "    # Make sure that the sample is sorted before calling this function\n",
    "    if sorted == False:\n",
    "        sample.sort()\n",
    "        \n",
    "    # Determine the descriptive statistics\n",
    "    n = len(sample)\n",
    "    maximum = max(sample)\n",
    "    minimum = min(sample)\n",
    "    mean = np.mean(sample)\n",
    "    median = np.median(sample)\n",
    "    sd = np.std(sample)\n",
    "    #kurtosis = scipy.stats.kurtosis(sample, fisher=False)\n",
    "    #skewness = scipy.stats.skew(sample)\n",
    "    \n",
    "    # Standardize the sample\n",
    "    standardized_sample = [(x - mean) / sd for x in sample]\n",
    "    \n",
    "    # Determine the number of quantiles\n",
    "    m = int(1 / q)\n",
    "    \n",
    "    # Determine the quantiles\n",
    "    # Note: Optimize this step. All the quantiles can be found in one single pass.\n",
    "    descriptor = [get_quantile(standardized_sample, j*q) for j in range(1, m + 1)]\n",
    "    \n",
    "    # Combine the quantiles and descriptive statistics to get the sample's descriptor\n",
    "    descriptor = descriptor + [n, mean, sd, minimum, maximum, median]\n",
    "    \n",
    "    return descriptor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, define the descriptor builder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-02T13:05:56.332267Z",
     "start_time": "2020-11-02T13:05:56.327351Z"
    }
   },
   "outputs": [],
   "source": [
    "class DescriptorBuilder(TransformerMixin, BaseEstimator):\n",
    "    def __init__(self, q=0.1):\n",
    "        super(DescriptorBuilder, self).__init__()\n",
    "        self.q = q\n",
    "        \n",
    "        # Set the names of the features in the descriptors\n",
    "        self.features = ['q{:.2f}'.format(i * q) for i in range(1, int(1/q) + 1)]\n",
    "        self.features += ['n', 'mean', 'sd', 'minimum', 'maximum', 'median']\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        # Not needed, but present for compatibility.\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        # Note: Currently works only on a list of lists or a single list.\n",
    "        if isinstance(X, list):\n",
    "            if all(isinstance(x, list) for x in X):\n",
    "                X = [get_descriptor(x, q=self.q, sorted=False) for x in X]\n",
    "                return pd.DataFrame(X)\n",
    "            else:\n",
    "                X = get_descriptor(X, q=self.q, sorted=False)\n",
    "                return X\n",
    "        else:\n",
    "            # Pandas dataframes and numpy arrays are not supported for now.\n",
    "            pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-validate\n",
    "\n",
    "If you want to use the already trained network, run the following cell to load the network and skip the rest of this Section. In the case you want to do the cross-validation yourself, skip the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-02T13:05:58.131184Z",
     "start_time": "2020-11-02T13:05:57.801497Z"
    }
   },
   "outputs": [],
   "source": [
    "path = os.path.join(classifiers_directory_path, 'dbnn_classifier.p')\n",
    "with open(path, 'rb') as f:\n",
    "    dbnn = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a `sklearn` pipeline that consists of the descriptor builder, standard scaler, mean imputer to replace the null values, and the neural network itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-23T23:58:38.591813Z",
     "start_time": "2020-09-23T23:58:38.580929Z"
    }
   },
   "outputs": [],
   "source": [
    "descriptor_builder = DescriptorBuilder()\n",
    "scaler = sklearn.preprocessing.StandardScaler()\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "neural_net = MLPClassifier(solver='adam', max_iter=200, activation='relu',\n",
    "                           random_state=random_state,\n",
    "                           early_stopping=True, validation_fraction=0.1)\n",
    "pipe = Pipeline([('descriptor_builder', descriptor_builder),\n",
    "                 ('scaler', scaler),\n",
    "                 ('imputer', imputer),\n",
    "                 ('neural_net', neural_net),\n",
    "                ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the grid search to fit the network's parameters and find the best hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-24T00:03:06.050985Z",
     "start_time": "2020-09-23T23:58:44.915436Z"
    }
   },
   "outputs": [],
   "source": [
    "# Specify the hyperparameter grid\n",
    "param_grid = dict(descriptor_builder__q = [0.05, 0.1],\n",
    "                  neural_net__hidden_layer_sizes = [(1000,), (100, 10)],\n",
    "                  neural_net__alpha = [1, 0.1],\n",
    "                 )\n",
    "\n",
    "# Define the grid search object\n",
    "grid = GridSearchCV(pipe,\n",
    "                    param_grid=param_grid,\n",
    "                    scoring='accuracy', # for evaluating predictions on the test sets\n",
    "                    refit=True,\n",
    "                    cv=cv_folds,\n",
    "                    verbose=1,\n",
    "                    n_jobs=-1)\n",
    "\n",
    "# Perform cross-validation\n",
    "grid.fit(set_A_cv, y_cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Show and Store the CV Results and Reports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the estimator obtained by fitting to the whole CV set using the best found hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-24T00:07:04.220694Z",
     "start_time": "2020-09-24T00:07:04.215562Z"
    }
   },
   "outputs": [],
   "source": [
    "# Get the trained network.\n",
    "dbnn = grid.best_estimator_\n",
    "dbnn_storage['classifier'] = dbnn\n",
    "\n",
    "# Show its hyperparameters\n",
    "grid.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show the CV results as a `pandas` dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-24T00:07:10.403416Z",
     "start_time": "2020-09-24T00:07:10.385150Z"
    }
   },
   "outputs": [],
   "source": [
    "# Show the means and deviations of the score(s) and CV time\n",
    "params = grid.cv_results_['params']\n",
    "mean_scores = grid.cv_results_['mean_test_score']\n",
    "score_sds = grid.cv_results_['std_test_score']\n",
    "mean_fit_times = grid.cv_results_['mean_fit_time']\n",
    "time_sds = grid.cv_results_['std_fit_time']\n",
    "\n",
    "results = []\n",
    "for (params, mean_score, score_sd, mean_fit_time, time_sd) in zip(params, mean_scores, score_sds, mean_fit_times, time_sds):\n",
    "    alpha = params['neural_net__alpha']\n",
    "    structure = params['neural_net__hidden_layer_sizes']\n",
    "    q = params['descriptor_builder__q']\n",
    "    results.append([q, str(structure), alpha, mean_score, score_sd, mean_fit_time, time_sd])\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.columns = ['q' ,'structure', 'c', 'mean_score', 'score_sd', 'mean_time', 'time_sd']\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show the results a $\\LaTeX$ table. For this, use `get_latex_table` and `mean_sd_merge` from `util`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-24T00:07:22.969538Z",
     "start_time": "2020-09-24T00:07:22.842555Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define the columns by which to sort the table\n",
    "sort_by = ['q', 'structure', 'c']\n",
    "\n",
    "# Define how to merge the means and standard deviations of the score and time\n",
    "merge_instructions = [{'merge_function' : util.mean_sd_merge, 'new_column' : 'accuracy',\n",
    "                       'columns_to_merge' : ['mean_score', 'score_sd']},\n",
    "                      {'merge_function' : util.mean_sd_merge, 'new_column' : 'time',\n",
    "                       'columns_to_merge' : ['mean_time', 'time_sd']}\n",
    "                     ]\n",
    "\n",
    "# Define how to rename the columns (leave None if no renaming is needed)\n",
    "renamer = None\n",
    "\n",
    "# Define the table's caption and label, as well as the format for the floats\n",
    "caption = 'Cross-validation results for the descriptor-based neural networks'\n",
    "label = 'tab:cv_dbnn'\n",
    "float_format = '$%.3f$'\n",
    "\n",
    "# Get the latex source\n",
    "latex = util.get_latex_table(results_df, sort_by=sort_by, merge_instructions=merge_instructions, renamer=renamer,\n",
    "                        caption=caption, label=label, float_format=float_format)\n",
    "# Show it\n",
    "print(latex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-24T00:07:23.513697Z",
     "start_time": "2020-09-24T00:07:23.505337Z"
    }
   },
   "outputs": [],
   "source": [
    "# Save the raw results and their dataframe\n",
    "dbnn_storage['results']['cv'] = grid.cv_results_\n",
    "dbnn_storage['reports']['cv'] = {'df' : results_df, 'latex' : latex}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check if the Descriptors are Well-formed\n",
    "\n",
    "Check importance of the features that the descriptors consist of. Use the set $\\mathcal{B}$ and the `Boruta` algorithm to do so.\n",
    "\n",
    "Define a function to perform all the steps in the `dbnn` pipeline save for making actual predictions with the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-02T13:06:22.602362Z",
     "start_time": "2020-11-02T13:06:22.598085Z"
    }
   },
   "outputs": [],
   "source": [
    "def prepare_input(X, dbnn_pipe):\n",
    "    X = dbnn_pipe['descriptor_builder'].transform(X)\n",
    "    X = dbnn_pipe['scaler'].transform(X)\n",
    "    X = dbnn_pipe['imputer'].transform(X)\n",
    "    \n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the `Boruta` algorithm on the set $\\mathcal{B}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-02T13:06:39.855175Z",
     "start_time": "2020-11-02T13:06:24.646129Z"
    }
   },
   "outputs": [],
   "source": [
    "# Prepare the set B\n",
    "set_B = datasets['B']['samples']\n",
    "labels = datasets['B']['labels']\n",
    "prepared_input = prepare_input(set_B, dbnn)\n",
    "\n",
    "# Define a random forest for the Boruta selection algorithm\n",
    "random_forest = RandomForestClassifier(n_jobs=-1, class_weight='balanced', max_depth=5)\n",
    "\n",
    "# Configure the Boruta algorithm...\n",
    "boruta_selector = boruta.BorutaPy(random_forest,\n",
    "                                  n_estimators='auto',\n",
    "                                  verbose=1,\n",
    "                                  max_iter=50,\n",
    "                                  perc=85,\n",
    "                                  alpha=0.05)\n",
    "# ... and run it.\n",
    "boruta_selector.fit(prepared_input, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show the selected features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-02T13:06:47.183640Z",
     "start_time": "2020-11-02T13:06:47.176563Z"
    }
   },
   "outputs": [],
   "source": [
    "selected_indices = np.where(boruta_selector.support_ == True)[0]\n",
    "selected_features = [dbnn['descriptor_builder'].features[i] for i in selected_indices]\n",
    "print(selected_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that no features were rejected. Now, check which ones were accepted and about which the algorithm was undecisive and were tentatively kept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-02T13:07:05.099444Z",
     "start_time": "2020-11-02T13:07:05.090956Z"
    }
   },
   "outputs": [],
   "source": [
    "accepted_indices = np.where(boruta_selector.ranking_ == 1)[0]\n",
    "print('Accepted features:', [dbnn['descriptor_builder'].features[i] for i in accepted_indices])\n",
    "\n",
    "tentative_indices = np.where(boruta_selector.ranking_ == 2)[0]\n",
    "print('Tentative features:', [dbnn['descriptor_builder'].features[i] for i in tentative_indices])\n",
    "\n",
    "rejected_indices = np.where(boruta_selector.ranking_ == 3)[0]\n",
    "print('Rejected features:', [dbnn['descriptor_builder'].features[i] for i in rejected_indices])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Calibration\n",
    "\n",
    "Check if the network is well-calibrated.\n",
    "\n",
    "Create a dataframe consisting of the network's predictes probabilities and actual labels of the samples in the set $\\mathcal{D}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-02T13:07:15.692431Z",
     "start_time": "2020-11-02T13:07:11.787965Z"
    }
   },
   "outputs": [],
   "source": [
    "set_D = prepare_input(datasets['D']['samples'], dbnn)\n",
    "\n",
    "probabilities = dbnn['neural_net'].predict_proba(set_D)\n",
    "results_D = pd.DataFrame({'nonnormal_prob' : probabilities[:, 0],\n",
    "                          'normal_prob' : probabilities[:, 1],\n",
    "                          'actual_label' : [int(label) for label in datasets['D']['labels']],\n",
    "                          #'predicted_label' : [int(label) for label in dbnn['neural_net'].predict(X)],\n",
    "                          'n' : [len(sample) for sample in datasets['D']['samples']],\n",
    "                         })\n",
    "results_D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To check for calibration, partition the probabilities into equal-sized bins and compare the proportions of the predicted normal samples with those of the actual normal samples in all the bins.\n",
    "\n",
    "First, define some utility functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-02T13:07:23.970156Z",
     "start_time": "2020-11-02T13:07:23.956865Z"
    }
   },
   "outputs": [],
   "source": [
    "def find_bin_index(x, bins):\n",
    "    for i in range(len(bins)):\n",
    "        (left, right) = bins[i]\n",
    "        if left <= x <= right:\n",
    "            return i\n",
    "\n",
    "def determine_predicted_proportions(df, bins):\n",
    "    df['bin'] = df['normal_prob'].apply(lambda x: find_bin_index(x, bins))\n",
    "    df['one'] = 1\n",
    "    \n",
    "    by_predicted = df[['bin', 'normal_prob']].groupby(by='bin').agg('mean')\n",
    "    predicted_proportions = by_predicted['normal_prob'].values\n",
    "\n",
    "    \n",
    "    return list(predicted_proportions)\n",
    "\n",
    "def determine_actual_proportions(df, bins):\n",
    "    df['bin'] = df['normal_prob'].apply(lambda x: find_bin_index(x, bins))\n",
    "    df['one'] = 1\n",
    "    \n",
    "    bin_sums = df[['bin', 'one']].groupby(by='bin').agg('sum')\n",
    "    \n",
    "    by_actual = df[['bin', 'actual_label', 'one']].groupby(by=['actual_label', 'bin'])\n",
    "    by_actual = by_actual.agg('count')\n",
    "\n",
    "    #for i in range(len(bins)):\n",
    "    #    if i not in bin_sums.index:\n",
    "    #        bin_sums.loc[i, 'one'] = 1\n",
    "    #        by_actual.loc[(1, i), 'one'] = 0\n",
    "    \n",
    "    actual_proportions=(by_actual / bin_sums).pipe(pd.DataFrame).loc[(1,)]['one'].values\n",
    "    \n",
    "    return list(actual_proportions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, define the bins and visualize the calibration plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-02T13:07:26.066166Z",
     "start_time": "2020-11-02T13:07:25.304347Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "n_bins = 10\n",
    "bin_size = 1.0 / n_bins\n",
    "bins = [((i - 1) * bin_size, i * bin_size) for i in range(1, n_bins + 1)]\n",
    "\n",
    "predicted = determine_predicted_proportions(results_D, bins)\n",
    "actual = determine_actual_proportions(results_D, bins)\n",
    "\n",
    "fig = plt.figure(figsize=(8,5))\n",
    "plt.plot(predicted, actual, color='navy', label='DBNN', linewidth=3)\n",
    "plt.plot(np.linspace(0, 1, 100), np.linspace(0, 1, 100), linestyle='--', linewidth=3,\n",
    "         color='crimson', label='Perfectly calibrated classifier')\n",
    "plt.legend(fontsize=13)\n",
    "\n",
    "dbnn_storage['reports']['evaluation']['calibration'] = {'fig' : fig}\n",
    "\n",
    "#for n in configuration['data']['datasets']['D']['n_range']:\n",
    "#    results_subframe = results_D[results_D['n'] == n].copy()\n",
    "#    \n",
    "#    predicted = determine_predicted_proportions(results_subframe, bins)\n",
    "#    actual = determine_actual_proportions(results_subframe, bins)\n",
    "#    fig = plt.figure(figsize=(8,5))\n",
    "#    plt.plot(predicted, actual, color='navy', label='DBNN')\n",
    "#    plt.plot(np.linspace(0, 1, 100), np.linspace(0, 1, 100), linestyle='--',\n",
    "#             color='crimson', label='Потпуно калибрисан класификатор')\n",
    "#    #plt.title('$n={}$'.format(n))\n",
    "#    plt.legend(fontsize=13)#\n",
    "#\n",
    "#    dbnn_storage['reports']['evaluation']['calibration']['fig_{}'.format(n)] = fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Present the variance by sampling from the dataset and vizualizing the calibration functions determined using the subsets. Plot the mean calibration function and check if it is close to the graph of the identity function, which represents a well-calibrated classifiers. Check if the sampled graphs capture that of the identity function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-02T13:07:37.704477Z",
     "start_time": "2020-11-02T13:07:34.057233Z"
    }
   },
   "outputs": [],
   "source": [
    "predicted_array = []\n",
    "actual_array = []\n",
    "for j in range(100):\n",
    "    subset = results_D.sample(n=1000)\n",
    "    proportions = determine_predicted_proportions(subset, bins)\n",
    "    #random_bins = draw_random_bins(max_size=0.2)\n",
    "    \n",
    "    actual = determine_actual_proportions(subset, bins)\n",
    "    #actual_array.append(actual)\n",
    "    \n",
    "    predicted = determine_predicted_proportions(results_D, bins)\n",
    "    #predicted_array.append(predicted)\n",
    "    \n",
    "    if len(actual) == len(predicted):\n",
    "        actual_array.append(actual)\n",
    "        predicted_array.append(predicted)\n",
    "    \n",
    "mean_predicted_proportions = pd.DataFrame(predicted_array).mean().values\n",
    "mean_actual_proportions = pd.DataFrame(actual_array).mean().values\n",
    "\n",
    "fig = plt.figure(figsize=(8, 5))\n",
    "\n",
    "plt.plot(np.linspace(0, 1, 100), np.linspace(0, 1, 100), linestyle='--',\n",
    "         color='crimson', label='Perfectly calibrated classifier',\n",
    "         zorder=3, linewidth=3)\n",
    "\n",
    "plt.plot(mean_predicted_proportions, mean_actual_proportions, color='navy', \n",
    "         zorder=2, linewidth=3, label='DBNN')\n",
    "\n",
    "for predicted, actual in zip(predicted_array, actual_array):\n",
    "    plt.plot(predicted, actual, color='lavender', zorder=1, alpha=1)\n",
    "\n",
    "plt.legend(fontsize=12)\n",
    "\n",
    "dbnn_storage['reports']['evaluation']['calibration']['mean_fig'] = fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Classification Performance\n",
    "\n",
    "Use the function `evaluate_pretty` from `util`. It can calculate the following metrics: accuracy, true positive rate, positive predictive value, F1 score, true negative rate, and negative predictive value (see [this Wikipedia article](https://en.wikipedia.org/wiki/Confusion_matrix) for definitions) for the whole set and by each sample size, returning the results as a nicely formatted `pandas` dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance on Set $\\mathcal{A}_{test}$\n",
    "\n",
    "Calculate and inspect the performance scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-02T13:07:52.433415Z",
     "start_time": "2020-11-02T13:07:50.233954Z"
    }
   },
   "outputs": [],
   "source": [
    "n_range = configuration['data']['datasets']['A']['n_range']\n",
    "metrics = ['A', 'TPR', 'PPV', 'TNR', 'NPV', 'F1', 'AUROC', 'UR']\n",
    "results_A_test = util.evaluate_pretty(set_A_test, y_test, dbnn,\n",
    "                                      n_range=n_range, index='n', metrics=metrics)\n",
    "\n",
    "#(A_test_results1.set_index('n', drop=True) - A_test_results.set_index('n', drop=True))#.lt(0).sum()\n",
    "dbnn_storage['results']['evaluation']['A_test'] = results_A_test\n",
    "\n",
    "latex = util.get_latex_table(results_A_test, index=True, caption='A', label='a')\n",
    "dbnn_storage['reports']['evaluation']['A_test'] = {'df' : results_A_test, 'latex' : latex}\n",
    "\n",
    "display(results_A_test)\n",
    "print(latex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-02T13:08:01.353304Z",
     "start_time": "2020-11-02T13:08:01.027613Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create a matplotlib figure\n",
    "metrics = ['TPR', 'PPV', 'NPV', 'TNR']\n",
    "mask = results_A_test.index != 'overall'\n",
    "df_to_plot = results_A_test[mask][metrics]\n",
    "\n",
    "fig = df_to_plot.plot(kind='line', linewidth=3, style=['--', '-.', '-', ':'],\n",
    "                      cmap=plt.get_cmap('coolwarm'),\n",
    "                      figsize=(10,7), use_index=True)\n",
    "plt.ylim(0, 1)\n",
    "plt.legend(fontsize=13)\n",
    "\n",
    "# Store it...\n",
    "dbnn_storage['reports']['evaluation']['A_test']['fig'] = fig\n",
    "\n",
    "# ... and show it.\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance on Set $\\mathcal{B}$\n",
    "\n",
    "Calculate and inspect the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-02T13:08:20.831301Z",
     "start_time": "2020-11-02T13:08:13.994546Z"
    }
   },
   "outputs": [],
   "source": [
    "n_range = configuration['data']['datasets']['B']['n_range']\n",
    "\n",
    "results_B = util.evaluate_pretty(datasets['B']['samples'],\n",
    "                                 datasets['B']['labels'], dbnn, n_range=n_range, index='n')\n",
    "\n",
    "latex = util.get_latex_table(results_B, index=True)\n",
    "\n",
    "dbnn_storage['results']['evaluation']['B'] = results_B\n",
    "dbnn_storage['reports']['evaluation']['B'] = {'df' : results_B, 'latex' : latex}\n",
    "\n",
    "display(results_B)\n",
    "print(latex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-02T13:09:35.016019Z",
     "start_time": "2020-11-02T13:09:34.848718Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create a matplotlib figure\n",
    "metrics = ['TPR', 'PPV', 'NPV', 'TNR']\n",
    "mask = results_B.index != 'overall'\n",
    "df_to_plot = results_B[mask][metrics]\n",
    "\n",
    "fig = df_to_plot.plot(kind='line', linewidth=3, style=['--', '-.', '-', ':'],\n",
    "                      cmap=plt.get_cmap('coolwarm'), \n",
    "                      figsize=(10,7), use_index=True)\n",
    "plt.ylim(0, 1)\n",
    "\n",
    "# Store it...\n",
    "dbnn_storage['reports']['evaluation']['B'] = {'fig' : fig}\n",
    "\n",
    "# ... and show it.\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compare Performance with that on $\\mathcal{A}_{test}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-02T13:09:44.307392Z",
     "start_time": "2020-11-02T13:09:43.884452Z"
    }
   },
   "outputs": [],
   "source": [
    "metrics_no_ur = ['A', 'TPR', 'PPV', 'TNR', 'NPV', 'AUROC', 'F1']\n",
    "df = pd.DataFrame({'$\\mathcal{A}_{test}$' : results_A_test.loc['overall', metrics_no_ur],\n",
    "                   '$\\mathcal{B}$' : results_B.loc['overall', metrics_no_ur]})\n",
    "\n",
    "fig = df.plot(kind='bar', cmap=plt.get_cmap('coolwarm'), \n",
    "              figsize=(10, 7), ylim=(0, 1), rot=0)\n",
    "\n",
    "#help(fig.legend)#(bbox_to_anchor=(1.2, 1.025))\n",
    "fig.legend(loc='upper center', bbox_to_anchor=(0.5, 1.1), ncol=2)\n",
    "\n",
    "dbnn_storage['reports']['evaluation']['AB'] = {'fig' : fig}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance on Set $\\mathcal{C}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-02T13:11:15.678622Z",
     "start_time": "2020-11-02T13:10:06.001745Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "n_range = range(10, 101, 10)\n",
    "metrics = ['TNR']\n",
    "for group in ['C-G1', 'C-G2', 'C-G3', 'C-G4']:\n",
    "    print(group)\n",
    "    \n",
    "    samples = datasets[group]['samples']\n",
    "    labels = datasets[group]['labels']\n",
    "\n",
    "    df = util.evaluate_pretty(samples, labels, dbnn, metrics=metrics, n_range=n_range, index='n')\n",
    "    #display(df)\n",
    "\n",
    "    dbnn_storage['results']['evaluation'][group] = df\n",
    "    \n",
    "    fig = df[df.index != 'overall'].plot(kind='line', cmap=plt.get_cmap('coolwarm'), linewidth=3,\n",
    "                                         figsize=(10,7), use_index=True)\n",
    "    plt.ylim(0, 1.05)\n",
    "    dbnn_storage['reports']['evaluation'][group] = {'fig' : fig}\n",
    "    #display(fig)\n",
    "    \n",
    "df = pd.DataFrame({group : dbnn_storage['results']['evaluation'][group]['TNR'] \\\n",
    "                   for group in ['C-G1', 'C-G2', 'C-G3', 'C-G4']})\n",
    "\n",
    "display(df)\n",
    "latex = util.get_latex_table(df, index=True)\n",
    "\n",
    "dbnn_storage['results']['evaluation']['C'] = {'df' : df}\n",
    "\n",
    "fig = df[df.index != 'overall'].plot(kind='line', cmap=plt.get_cmap('coolwarm'),\n",
    "                                     figsize=(10,7), use_index=True, linewidth=3,\n",
    "                                    style=['o-', 's-', 'v-', '^-'])\n",
    "plt.ylim(0, 1.05)\n",
    "latex = util.get_latex_table(df, index=True, caption='C', label='c')\n",
    "dbnn_storage['reports']['evaluation']['C'] = {'latex' : latex, 'fig' : fig}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare DBNN with the Selected Standard Statistical Tests of Normality\n",
    "\n",
    "Create classifiers based on the selected normality tests and nominal TPR levels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-02T13:11:30.365292Z",
     "start_time": "2020-11-02T13:11:30.358672Z"
    }
   },
   "outputs": [],
   "source": [
    "test_classifiers = {}\n",
    "\n",
    "codes = ['SW', 'LF', 'JB', 'AD', 'FSSD']\n",
    "\n",
    "for test_code in codes:\n",
    "    test, statistic = util.get_test(test_code)\n",
    "    for alpha in [0.01, 0.05, 0.1]:\n",
    "        test_classifiers[(test_code, alpha)] = util.TestClassifier(test, statistic, alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Include the statistic-based neural network, even though it technically is not a statistical test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-02T13:11:32.132856Z",
     "start_time": "2020-11-02T13:11:32.120566Z"
    }
   },
   "outputs": [],
   "source": [
    "with open(os.path.join('classifiers', 'sbnn.p'), 'rb') as f:\n",
    "    sbnn = pickle.load(f)\n",
    "\n",
    "test_classifiers[('SBNN', '')] = sbnn\n",
    "codes += ['SBNN']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare the TNR Scores Using the Set $\\mathcal{C}$\n",
    "\n",
    "Note that NPV is 1.0 for all the tests, as well as the DBNN, so we need to look at the TNR values only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-02T14:06:59.411060Z",
     "start_time": "2020-11-02T13:11:37.815704Z"
    }
   },
   "outputs": [],
   "source": [
    "n_range = range(10, 101, 10)\n",
    "metrics = ['TNR']\n",
    "\n",
    "for group in ['C-G1', 'C-G2', 'C-G3', 'C-G4']:\n",
    "    print(group)\n",
    "    \n",
    "    samples = datasets[group]['samples']\n",
    "    labels = datasets[group]['labels']\n",
    "    \n",
    "    all_test_results = {}\n",
    "    for alpha in [0.1]:\n",
    "        all_test_results[alpha] = {}\n",
    "        for test_code in codes:\n",
    "            print('\\t', test_code)\n",
    "            if test_code != 'SBNN':\n",
    "                test_clf = test_classifiers[(test_code, alpha)]\n",
    "            else:\n",
    "                test_clf = test_classifiers[(test_code, '')]\n",
    "            \n",
    "            test_results_df = util.evaluate_pretty(samples, labels, test_clf, metrics=metrics, n_range=n_range, index='n')\n",
    "            all_test_results[alpha][test_code] = test_results_df\n",
    "            \n",
    "    dbnn_storage['results']['comparison'][group] = all_test_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-02T14:12:52.700286Z",
     "start_time": "2020-11-02T14:12:52.420138Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "C_results = {}\n",
    "for group in dbnn_storage['results']['comparison']:\n",
    "    print(group)\n",
    "    results = dbnn_storage['results']['comparison'][group]\n",
    "    results_dict = {'{}({})'.format(test_code, alpha): results[alpha][test_code]['TNR'] for alpha in results\\\n",
    "                    for test_code in results[alpha]}\n",
    "    results_df = pd.concat(results_dict, axis=1)\n",
    "    results_df = results_df[sorted(results_df.columns)]\n",
    "    results_df['DBNN'] = dbnn_storage['results']['evaluation'][group]['TNR']\n",
    "    results_df = results_df[['DBNN'] + [col for col in results_df.columns if col != 'DBNN']]\n",
    "    C_results[group] = results_df\n",
    "    display(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-02T14:13:09.056565Z",
     "start_time": "2020-11-02T14:13:07.486719Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for group in C_results:\n",
    "    df = C_results[group]\n",
    "    fig = df[df.index != 'overall'].plot(kind='line', style=['o-', 'v--', '^--', 's--', 'D--', 'p--', 'x--'],\n",
    "                                         #color=['navy', 'darkred', 'red', 'orangered', 'orange'],\n",
    "                                         figsize=(10,7), use_index=True)\n",
    "    latex = util.get_latex_table(df, index=True, caption='C', label='c')\n",
    "    dbnn_storage['reports']['comparison'][group] = {'fig' : fig, 'latex': latex}\n",
    "    print(latex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare Performance Using Set $\\mathcal{D}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-02T14:24:03.550260Z",
     "start_time": "2020-11-02T14:13:29.423513Z"
    }
   },
   "outputs": [],
   "source": [
    "samples = datasets['D']['samples']\n",
    "labels = datasets['D']['labels']\n",
    "\n",
    "dbnn_storage['results']['comparison']['D'] = {}\n",
    "\n",
    "metrics = ['A', 'TPR', 'PPV', 'TNR', 'NPV', 'F1', 'UR']\n",
    "\n",
    "for (code, alpha) in test_classifiers:\n",
    "    print(code, alpha)\n",
    "    classifier = test_classifiers[(code, alpha)]\n",
    "    results_df = util.evaluate_pretty(samples, labels, classifier, metrics=metrics)\n",
    "    dbnn_storage['results']['comparison']['D'][(code, alpha)] = results_df\n",
    "\n",
    "dbnn_storage['results']['comparison']['D'][('DBNN', '')] = util.evaluate_pretty(samples, labels, dbnn,\n",
    "                                                                               metrics=metrics)\n",
    "\n",
    "df_report = pd.concat(dbnn_storage['results']['comparison']['D'])\n",
    "df_report.index=pd.MultiIndex.from_tuples([(x[0], x[1]) for x in df_report.index])\n",
    "\n",
    "dbnn_storage['reports']['comparison']['D'] = {\n",
    "    'df' : df_report,\n",
    "    'latex' : util.get_latex_table(df_report, index=True)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show the dataframe with the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-02T14:24:43.992323Z",
     "start_time": "2020-11-02T14:24:43.972983Z"
    }
   },
   "outputs": [],
   "source": [
    "df_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show the LaTeX table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-02T14:24:52.239558Z",
     "start_time": "2020-11-02T14:24:52.228017Z"
    }
   },
   "outputs": [],
   "source": [
    "print(util.get_latex_table(df_report, index=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conduct ROC Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the ROC curves and calculate the areas under them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-02T14:28:17.319909Z",
     "start_time": "2020-11-02T14:24:59.294902Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "samples = datasets['D']['samples']\n",
    "labels = datasets['D']['labels']\n",
    "\n",
    "fig = plt.figure(figsize=(10, 7))\n",
    "styles = ['dotted', 'dashed', 'dashdot', (0, (3, 5, 1, 5, 1, 5))] * 4\n",
    "\n",
    "aurocs = {}\n",
    "\n",
    "print('DBNN')\n",
    "prepared_input = prepare_input(samples, dbnn)\n",
    "probabilities = dbnn['neural_net'].predict_proba(prepared_input)\n",
    "scores = probabilities[:, 1]\n",
    "mask = np.isfinite(np.array(scores))    \n",
    "filtered_labels = np.array(labels)[mask]\n",
    "filtered_scores = np.array(scores)[mask]\n",
    "fpr, tpr, tr = sklearn.metrics.roc_curve(filtered_labels, filtered_scores, pos_label=1)\n",
    "plt.plot(fpr, tpr, label='DBNN', linestyle='-', linewidth=3)\n",
    "aurocs['DBNN'] = sklearn.metrics.roc_auc_score(filtered_labels, filtered_scores)\n",
    "\n",
    "for code in ['LF', 'JB', 'AD', 'SW', 'FSSD']:\n",
    "    alpha = 0.01\n",
    "    print(code)\n",
    "    _, statistic = util.get_test(code)\n",
    "    scores = np.array([statistic(sample) for sample in samples])\n",
    "    \n",
    "    if code in ['LF', 'JB', 'AD', 'FSSD']:\n",
    "        scores = -scores\n",
    "    \n",
    "    mask = np.isfinite(scores)   \n",
    "    filtered_labels = np.array(labels)[mask]\n",
    "    filtered_scores = np.array(scores)[mask]\n",
    "    \n",
    "    fpr, tpr, tr = sklearn.metrics.roc_curve(filtered_labels, filtered_scores, pos_label=1)\n",
    "    \n",
    "    plt.plot(fpr, tpr, label=code, linestyle=styles.pop(0), linewidth=3)\n",
    "    aurocs[code] = sklearn.metrics.roc_auc_score(filtered_labels, filtered_scores)\n",
    "    \n",
    "\n",
    "\n",
    "print('SBNN')\n",
    "data = sbnn['preprocessor'].transform(samples)\n",
    "data = sbnn['scaler'].transform(data)\n",
    "prepared_input = sbnn['imputer'].transform(data)\n",
    "probabilities = sbnn['neural_net'].predict_proba(prepared_input)\n",
    "scores = probabilities[:, 1]\n",
    "mask = np.isfinite(np.array(scores))    \n",
    "filtered_labels = np.array(labels)[mask]\n",
    "filtered_scores = np.array(scores)[mask]\n",
    "fpr, tpr, tr = sklearn.metrics.roc_curve(filtered_labels, filtered_scores, pos_label=1)\n",
    "plt.plot(fpr, tpr, label='SBNN', linestyle=styles.pop(0), linewidth=3)\n",
    "aurocs['SBNN'] = sklearn.metrics.roc_auc_score(filtered_labels, filtered_scores)\n",
    "\n",
    "plt.xlabel('FPR')\n",
    "plt.ylabel('TPR')\n",
    "plt.legend(fontsize=13)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show the values of the AUROC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-02T14:28:17.324438Z",
     "start_time": "2020-11-02T14:28:17.321139Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "aurocs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Store the calculated values and the figure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-02T14:29:56.730602Z",
     "start_time": "2020-11-02T14:29:56.723485Z"
    }
   },
   "outputs": [],
   "source": [
    "dbnn_storage['results']['comparison']['D'] = {}\n",
    "dbnn_storage['results']['comparison']['D']['auroc'] = aurocs\n",
    "dbnn_storage['reports']['comparison']['D']['auroc_fig'] = fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison on Real-World Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### $\\mathcal{R}_{height}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-02T14:30:01.955454Z",
     "start_time": "2020-11-02T14:30:00.900204Z"
    }
   },
   "outputs": [],
   "source": [
    "samples = datasets['R_height']['samples']\n",
    "\n",
    "samples_lt10 = [sample for sample in samples if len(sample) < 10]\n",
    "m_lt10 = len(samples_lt10)\n",
    "samples_ge10 = [sample for sample in samples if len(sample) >= 10]\n",
    "m_ge10 = len(samples_ge10)\n",
    "\n",
    "results = []\n",
    "\n",
    "s_lt10 = sum(dbnn.predict(samples_lt10))\n",
    "s_ge10 = sum(dbnn.predict(samples_ge10))\n",
    "results.append([('DBNN', ''), s_lt10 / m_lt10, s_ge10 / m_ge10])\n",
    "\n",
    "for code in test_classifiers:\n",
    "    classifier = test_classifiers[code]\n",
    "    \n",
    "    s_lt10 = sum(classifier.predict(samples_lt10))\n",
    "    s_ge10 = sum(classifier.predict(samples_ge10))\n",
    "    \n",
    "    results.append([code, s_lt10 / m_lt10, s_ge10 / m_ge10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-02T14:30:03.351379Z",
     "start_time": "2020-11-02T14:30:03.182777Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(results)\n",
    "df.columns = ['clf', '$n < 10$', '$n \\geq 10$']\n",
    "df = df.set_index(pd.MultiIndex.from_tuples(df['clf']))\n",
    "df = df.drop('clf', axis='columns')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-02T14:30:24.137199Z",
     "start_time": "2020-11-02T14:30:24.127521Z"
    }
   },
   "outputs": [],
   "source": [
    "latex = util.get_latex_table(df, index=True, caption='caption', label='label')\n",
    "print(latex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### $\\mathcal{R}_{earthquake}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-02T14:30:28.997526Z",
     "start_time": "2020-11-02T14:30:28.985850Z"
    }
   },
   "outputs": [],
   "source": [
    "samples = datasets['R_earthquake']['samples']\n",
    "labels = datasets['R_earthquake']['labels']\n",
    "data = util.separate_by_size(samples)\n",
    "ns = sorted(data.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-02T14:37:16.130082Z",
     "start_time": "2020-11-02T14:30:29.765482Z"
    }
   },
   "outputs": [],
   "source": [
    "results = []\n",
    "for n in ns:\n",
    "    m = len(data[n])\n",
    "    row = [n]\n",
    "    s = m - sum(dbnn.predict(data[n]))\n",
    "    row.append(s / m)\n",
    "    for code in test_classifiers:\n",
    "        classifier = test_classifiers[code]\n",
    "        s = m - sum(classifier.predict(data[n]))\n",
    "        row.append(s / m)\n",
    "    results.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-02T14:37:16.158982Z",
     "start_time": "2020-11-02T14:37:16.131490Z"
    }
   },
   "outputs": [],
   "source": [
    "codes = list(test_classifiers.keys())\n",
    "results_df = pd.DataFrame(results, columns=['n', ('DBNN', '')] + codes)\n",
    "results_df = results_df.set_index('n')\n",
    "results_df.columns = pd.MultiIndex.from_tuples(results_df.columns)\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-02T14:37:16.381514Z",
     "start_time": "2020-11-02T14:37:16.160285Z"
    }
   },
   "outputs": [],
   "source": [
    "latex = util.get_latex_table(results_df, index=True, caption='A', label='a')\n",
    "print(latex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Runtime Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-02T14:41:41.075722Z",
     "start_time": "2020-11-02T14:41:41.068450Z"
    }
   },
   "outputs": [],
   "source": [
    "descriptor_builder = DescriptorBuilder(q=0.1)\n",
    "scaler = sklearn.preprocessing.StandardScaler()\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "neural_net = MLPClassifier(solver='adam', max_iter=200, activation='relu',\n",
    "                           hidden_layer_sizes = (100, 10),\n",
    "                           alpha = 0.1,\n",
    "                           early_stopping=True, validation_fraction=0.1)\n",
    "pipe = Pipeline([('descriptor_builder', descriptor_builder),\n",
    "                 ('scaler', scaler),\n",
    "                 ('imputer', imputer),\n",
    "                 ('neural_net', neural_net),\n",
    "                ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-02T14:47:25.868340Z",
     "start_time": "2020-11-02T14:42:34.522817Z"
    }
   },
   "outputs": [],
   "source": [
    "train_sizes, train_scores, test_scores, fit_times, _ = learning_curve(pipe, set_A_cv, y_cv, cv=20, n_jobs=-1,\n",
    "                                                                      train_sizes=np.linspace(0.1, 1.0, 5),\n",
    "                                                                      return_times=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-02T14:48:22.946110Z",
     "start_time": "2020-11-02T14:48:22.938322Z"
    }
   },
   "outputs": [],
   "source": [
    "dbnn_storage['reports']['evaluation']['runtime'] = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-02T14:48:24.909199Z",
     "start_time": "2020-11-02T14:48:24.881639Z"
    }
   },
   "outputs": [],
   "source": [
    "np.median(train_scores, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-02T14:49:45.730091Z",
     "start_time": "2020-11-02T14:49:45.312912Z"
    }
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 7))\n",
    "\n",
    "#for tr_s in train_scores.T:\n",
    "#    plt.plot(train_sizes, tr_s, color='navy', alpha=0.1, linewidth=3, linestyle='-', zorder=2)\n",
    "\n",
    "plt.plot(train_sizes, np.median(train_scores, axis=1), color='navy', linewidth=3, linestyle='--', zorder=3)   \n",
    "\n",
    "#for te_s in test_scores.T:\n",
    "#    plt.plot(train_sizes, te_s, color='mistyrose', linewidth=3, linestyle='-')\n",
    "    \n",
    "plt.plot(train_sizes, np.median(test_scores, axis=1), color='crimson', linewidth=3, linestyle=':', zorder=3) \n",
    "\n",
    "legend_objects = [Line2D([0], [0], color='navy', lw=3, ls='--'), Line2D([0], [0], color='crimson', lw=3, ls=':')]\n",
    "plt.legend(legend_objects, ['Training', 'Test'], fontsize=13)\n",
    "    \n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Number of training samples')\n",
    "plt.ylim(0.85, 0.95)\n",
    "\n",
    "dbnn_storage['reports']['evaluation']['runtime']['train_test_for_size_fig'] = fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-02T14:49:51.278449Z",
     "start_time": "2020-11-02T14:49:50.607367Z"
    }
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 7))\n",
    "\n",
    "\n",
    "plt.scatter(np.repeat(train_sizes, 20), fit_times, color='navy', alpha=0.5)\n",
    "plt.plot(train_sizes, np.median(fit_times, axis=1), color='navy', linewidth=3)\n",
    "\n",
    "plt.xlabel('Number of training samples')\n",
    "plt.ylabel('Fit time (s)')\n",
    "\n",
    "dbnn_storage['reports']['evaluation']['runtime']['fit_time_size_fig'] = fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation on Larger Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-02T14:50:21.823095Z",
     "start_time": "2020-11-02T14:50:03.474578Z"
    }
   },
   "outputs": [],
   "source": [
    "n_range = [250, 500, 1000]\n",
    "s_range = [x/10.0 for x in range(-300, 301, 5)]\n",
    "k_range = [x/10.0 for x in range(0, 401, 5)] \n",
    "# Generate non-normal samples\n",
    "nonnormal_samples = util.generate_pearson_nonnormal_samples(s_range, k_range, n_range, 2)\n",
    "\n",
    "# Calculate L, the number of normal samples of the same size\n",
    "l = len(nonnormal_samples) // len(n_range)\n",
    "            \n",
    "# Generate L normal samples of size n for each n in n_range\n",
    "normal_samples = util.generate_normal_samples(n_range, l)\n",
    "\n",
    "# Label the samples\n",
    "#normal_samples = util.label_samples(normal_samples, 1)\n",
    "#nonnormal_samples = util.label_samples(nonnormal_samples, 0)\n",
    "\n",
    "# Unify them\n",
    "large_samples = normal_samples + nonnormal_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-02T14:50:45.472911Z",
     "start_time": "2020-11-02T14:50:21.824473Z"
    }
   },
   "outputs": [],
   "source": [
    "labels = [1 for x in normal_samples] + [0 for x in nonnormal_samples]\n",
    "df = util.evaluate_pretty(large_samples, labels, dbnn, n_range=n_range, index='n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-02T14:50:45.493695Z",
     "start_time": "2020-11-02T14:50:45.478578Z"
    }
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-02T14:54:00.173193Z",
     "start_time": "2020-11-02T14:53:27.606585Z"
    }
   },
   "outputs": [],
   "source": [
    "# Find optimal thresholds\n",
    "\n",
    "optimal_tr = {}\n",
    "for n in [250, 500, 1000]:\n",
    "    # Generate non-normal samples\n",
    "    nonnormal_samples_1 = util.generate_pearson_nonnormal_samples(s_range, k_range, [n], 2)\n",
    "\n",
    "    # Calculate L, the number of normal samples of the same size\n",
    "    L = len(nonnormal_samples_1) // 1\n",
    "            \n",
    "    # Generate L normal samples of size n for each n in n_range\n",
    "    normal_samples_1 = util.generate_normal_samples([n], L)\n",
    "\n",
    "    # Unify them\n",
    "    large_samples_1 = normal_samples_1 + nonnormal_samples_1\n",
    "    \n",
    "    labels_1 = [1 for x in normal_samples_1] + [0 for x in nonnormal_samples_1]\n",
    "    input_samples = prepare_input(large_samples_1, dbnn)\n",
    "    \n",
    "    scores = dbnn['neural_net'].predict_proba(input_samples)[:, 1]\n",
    "    \n",
    "    print(len(large_samples_1), len(labels_1), input_samples.shape, scores.shape)\n",
    "    \n",
    "    fprs, tprs, trs = sklearn.metrics.roc_curve(labels_1, scores)\n",
    "    \n",
    "    best = np.inf\n",
    "    optimal_tr[n] = None\n",
    "\n",
    "    for (fpr, tpr, tr) in zip(fprs, tprs, trs):\n",
    "        if fpr + 1 - tpr < best:\n",
    "            best = fpr + 1 - tpr\n",
    "            optimal_tr[n] = tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-02T14:54:00.207774Z",
     "start_time": "2020-11-02T14:54:00.174998Z"
    }
   },
   "outputs": [],
   "source": [
    "optimal_tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-02T14:54:00.323909Z",
     "start_time": "2020-11-02T14:54:00.209955Z"
    }
   },
   "outputs": [],
   "source": [
    "class AdHocClassifier(object):\n",
    "    def __init__(self, thresholds):\n",
    "        super(AdHocClassifier).__init__()\n",
    "        self.thresholds = thresholds\n",
    "    \n",
    "    def predict(self, data):\n",
    "        thresholds = [self.thresholds[len(sample)] for sample in data]\n",
    "        input_data = prepare_input(data, dbnn)\n",
    "        scores = dbnn['neural_net'].predict_proba(input_data)[:, 1]\n",
    "        guesses = scores > thresholds\n",
    "        return guesses + 0\n",
    "    \n",
    "    def predict_proba(self, data):\n",
    "        input_data = prepare_input(data, dbnn)\n",
    "        return dbnn['neural_net'].predict_proba(input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-02T14:54:24.577197Z",
     "start_time": "2020-11-02T14:54:00.328866Z"
    }
   },
   "outputs": [],
   "source": [
    "ahc = AdHocClassifier(optimal_tr)\n",
    "df = util.evaluate_pretty(large_samples, labels, ahc, n_range=n_range, index='n')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save\n",
    "\n",
    "Save the classifier, the results and reports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = grid.best_estimator_\n",
    "path = os.path.join(classifiers_directory_path, 'dbnn.p')\n",
    "with open(path, 'wb') as f:\n",
    "    pickle.dump(classifier, f)\n",
    "    \n",
    "results_directory_path = configuration['results']['directory_path']\n",
    "path = os.path.join(results_directory_path, 'dbnn_results.p')\n",
    "with open(path, 'wb') as f:\n",
    "    pickle.dump(dbnn_storage['results'], f)\n",
    "\n",
    "reports_directory_path = configuration['reports']['directory_path']\n",
    "path = os.path.join(reports_directory_path, 'dbnn')\n",
    "pathlib.Path(*path.split(os.sep)).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "reports_directory_path = path\n",
    "path = os.path.join(reports_directory_path, 'dbnn_reports.p')\n",
    "with open(path, 'wb') as f:\n",
    "    pickle.dump(dbnn_storage['reports'], f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "Save the images as PDF files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-07T12:20:25.617611Z",
     "start_time": "2020-09-07T12:20:25.610972Z"
    }
   },
   "outputs": [],
   "source": [
    "def traverse_and_save(dictionary, img_dir_path):\n",
    "    if type(dictionary) is not dict:\n",
    "        return\n",
    "    for key in dictionary:\n",
    "        if 'fig' in key:\n",
    "            pathlib.Path(*img_dir_path.split(os.sep)).mkdir(parents=True, exist_ok=True)\n",
    "            figure = dictionary[key]\n",
    "            path = os.path.join(img_dir_path, img_dir_path.split(os.sep)[-1] + '_' + key) + '.pdf'\n",
    "            print('Saving', path)\n",
    "            if 'savefig' in dir(figure):\n",
    "                figure.savefig(path, bbox_inches='tight')\n",
    "            else:\n",
    "                figure.figure.savefig(path, bbox_inches='tight')\n",
    "        else:\n",
    "            traverse_and_save(dictionary[key], os.path.join(img_dir_path, key))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traverse_and_save(dbnn_storage['reports'], reports_directory_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the updated configuration dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(config_path, 'wb') as f:\n",
    "    pickle.dump(configuration, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "p3normal",
   "language": "python",
   "name": "p3normal"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "486.217px",
    "left": "1024px",
    "top": "140.533px",
    "width": "339.9px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
