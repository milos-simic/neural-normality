{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistic-Based Neural Network for Normality Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-02T12:55:50.485876Z",
     "start_time": "2020-11-02T12:55:50.478414Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import everything that's needed to run the notebook\n",
    "import os\n",
    "import pickle\n",
    "import pathlib\n",
    "import datetime\n",
    "import random\n",
    "\n",
    "from IPython.display import display, Markdown, Latex\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "import sklearn.metrics\n",
    "import sklearn.preprocessing\n",
    "import scipy.stats\n",
    "import matplotlib.pyplot as plt\n",
    "import boruta\n",
    "\n",
    "import util"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-02T12:55:51.852720Z",
     "start_time": "2020-11-02T12:55:51.846665Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define the path to the configuration dictionary\n",
    "config_path = 'configuration.p'\n",
    "\n",
    "# Load the configuration dictionary\n",
    "with open(config_path, 'rb') as f:\n",
    "    configuration = pickle.load(f)\n",
    "    \n",
    "# Get the paths to the relevant directories \n",
    "data_directory_path = configuration['data']['directory_path']\n",
    "classifiers_directory_path = configuration['classifiers']['directory_path']\n",
    "\n",
    "# Get the parameters of the experiment\n",
    "cv_folds = configuration['experiment']['number_of_cv_folds']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the data\n",
    "Load the datasets using the function `load_from_file` from `util`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-02T12:55:55.298581Z",
     "start_time": "2020-11-02T12:55:54.785787Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define the dictionary to store the actual datasets, indexed by their names\n",
    "datasets = {}\n",
    "\n",
    "# Load the datasets\n",
    "for set_name in ['A']:\n",
    "    set_path = configuration['data']['datasets'][set_name]['path']\n",
    "    print('Loading {} from {}'.format(set_name, set_path))\n",
    "    datasets[set_name] = util.load_from_file(set_path)\n",
    "    print('Done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split $\\mathcal{A}$ into Cross-validation and Test  Subsets ($\\mathcal{A}_{cv}$ and $\\mathcal{A}_{test}$) \n",
    "\n",
    "Split $\\mathcal{A}$ into the subsets for cross-validation ($\\mathcal{A}_{cv})$ and testing ($\\mathcal{A}_{test}$).\n",
    "\n",
    "Use 70% of the set to cross-validate and 30% for subsequent testing and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-02T12:55:57.038276Z",
     "start_time": "2020-11-02T12:55:56.727893Z"
    }
   },
   "outputs": [],
   "source": [
    "# Extract the labels from the set, leaving only samples in it\n",
    "labels = [labeled_sample.pop() for labeled_sample in datasets['A']]\n",
    "samples = datasets['A']\n",
    "\n",
    "# There is no need to store the sama data twice, in datasets['A'] and in (samples, labels)\n",
    "del datasets['A']\n",
    "\n",
    "# Define the stratification labels as the combination of actual labels and sample sizes\n",
    "stratification_labels = [str(label) + str(len(sample)) for (label, sample) in zip(labels, samples)]\n",
    "\n",
    "# Set the relative size of the CV subset\n",
    "train_size = 0.7\n",
    "\n",
    "# Split the data into CV and test subsets\n",
    "set_A_cv, set_A_test, y_cv, y_test = train_test_split(samples, labels, stratify=stratification_labels, \n",
    "                                              train_size=train_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct the Statistic-based Feedforward Neural Network Classifier (SBNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the Preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-02T12:56:00.437506Z",
     "start_time": "2020-11-02T12:56:00.426721Z"
    }
   },
   "outputs": [],
   "source": [
    "def lin_mudholkar_statistic(sample, tol=1e-7):\n",
    "    n = len(sample)\n",
    "    sum_of_squares = 0\n",
    "    for x in sample:\n",
    "        sum_of_squares = sum_of_squares + x**2\n",
    "        \n",
    "    h_values = [0 for i in range(n)]\n",
    "    for i in range(n):\n",
    "        x = sample[i]\n",
    "        \n",
    "        corrected_sum = 0\n",
    "        for j in range(n):\n",
    "            if j != i:\n",
    "                corrected_sum = corrected_sum + sample[j]\n",
    "                \n",
    "        square_of_sum = corrected_sum**2\n",
    "        difference = (((sum_of_squares - x**2) - square_of_sum / (n - 1)) / n)\n",
    "        if abs(difference) <= tol:\n",
    "            difference = 0\n",
    "        h_i = difference**(1/3)\n",
    "        h_values[i] = h_i\n",
    "        \n",
    "        #print(i, x, corrected_sum, square_of_sum, square_of_sum / (n - 1), '\\n', h_i)\n",
    "    \n",
    "    r = np.corrcoef(sample, h_values)\n",
    "    \n",
    "    return np.arctan(r[0, 1])\n",
    "\n",
    "def vasicek_statistic(sample, m=3):\n",
    "    n = len(sample)\n",
    "    sample = np.array(sample, dtype=np.float64)\n",
    "    sd = np.std(sample)\n",
    "    sorted_sample = np.sort(sample)\n",
    "    product = 1\n",
    "    m = m\n",
    "    for i in range(n):\n",
    "        if i - m < 0:\n",
    "            left = sorted_sample[0]\n",
    "        else:\n",
    "            left = sorted_sample[i - m]\n",
    "        \n",
    "        if i + m > n - 1:\n",
    "            right = sorted_sample[n - 1]\n",
    "        else:\n",
    "            right = sorted_sample[i + m]\n",
    "        \n",
    "        product = product * (right - left)\n",
    "    \n",
    "    return (n / (2 * m * sd)) * (product ** (1 / n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-02T12:56:01.182253Z",
     "start_time": "2020-11-02T12:56:01.162487Z"
    }
   },
   "outputs": [],
   "source": [
    "def preprocess(sample):\n",
    "    n = len(sample)\n",
    "    skewness = scipy.stats.skew(sample)\n",
    "    kurtosis = scipy.stats.kurtosis(sample, fisher=False)\n",
    "    W = scipy.stats.shapiro(sample).statistic\n",
    "    lm_stat = lin_mudholkar_statistic(sample)\n",
    "    K3 = vasicek_statistic(sample, m=3)\n",
    "    K5 = vasicek_statistic(sample, m=5)\n",
    "    return [skewness, kurtosis, W, lm_stat, K3, K5, n]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-02T12:56:02.087974Z",
     "start_time": "2020-11-02T12:56:02.079660Z"
    }
   },
   "outputs": [],
   "source": [
    "class SBNNPreprocessor(TransformerMixin, BaseEstimator):\n",
    "    def __init__(self):\n",
    "        super(SBNNPreprocessor, self).__init__()\n",
    "        \n",
    "        # Set the names of the features in the descriptors\n",
    "        self.features = ['skewness', 'kurtosis', 'W', 'LN-statistic', 'K3', 'K5', 'n']\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        # Not needed, but present for compatibility.\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        # Note: Currently working only on a list of lists or a single list.\n",
    "        if isinstance(X, list):\n",
    "            if all(isinstance(x, list) for x in X):\n",
    "                X = [preprocess(x) for x in X]\n",
    "                return pd.DataFrame(X)\n",
    "            else:\n",
    "                X = preprocess(X)\n",
    "                return X\n",
    "        else:\n",
    "            # Pandas dataframes and numpy arrays are not supported for now.\n",
    "            pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-validate\n",
    "Create a `sklearn` pipeline that consists of the preprocessor, standard scaler, mean imputer to replace the null values, and the neural network itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-02T12:56:03.691838Z",
     "start_time": "2020-11-02T12:56:03.686968Z"
    }
   },
   "outputs": [],
   "source": [
    "preprocessor = SBNNPreprocessor()\n",
    "scaler = sklearn.preprocessing.StandardScaler()\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "neural_net = MLPClassifier(solver='adam', max_iter=200, activation='relu',\n",
    "                           early_stopping=True, validation_fraction=0.1)\n",
    "pipe = Pipeline([('preprocessor', preprocessor),\n",
    "                 ('scaler', scaler),\n",
    "                 ('imputer', imputer),\n",
    "                 ('neural_net', neural_net),\n",
    "                ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the grid search to fit the network's parameters and find the best hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-02T13:01:43.097149Z",
     "start_time": "2020-11-02T12:56:05.498419Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Specify the hyperparameter grid\n",
    "param_grid = dict(\n",
    "                  neural_net__hidden_layer_sizes = [(1000), (100, 10)],\n",
    "                  neural_net__alpha = [10, 1, 0.1],\n",
    "                 )\n",
    "\n",
    "# Define the grid search object\n",
    "grid = GridSearchCV(pipe,\n",
    "                    param_grid=param_grid,\n",
    "                    scoring='accuracy',\n",
    "                    refit=True,\n",
    "                    cv=cv_folds,\n",
    "                    verbose=1,\n",
    "                    n_jobs=-1)\n",
    "\n",
    "# Perform cross-validation\n",
    "grid.fit(set_A_cv, y_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-02T13:01:43.103126Z",
     "start_time": "2020-11-02T13:01:43.099038Z"
    }
   },
   "outputs": [],
   "source": [
    "# Get the trained network.\n",
    "sbnn = grid.best_estimator_\n",
    "\n",
    "# Show its hyperparameters\n",
    "grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-02T13:01:51.133828Z",
     "start_time": "2020-11-02T13:01:50.982991Z"
    }
   },
   "outputs": [],
   "source": [
    "# Show the means and deviations of the score(s) and CV time\n",
    "params = grid.cv_results_['params']\n",
    "mean_scores = grid.cv_results_['mean_test_score']\n",
    "score_sds = grid.cv_results_['std_test_score']\n",
    "mean_fit_times = grid.cv_results_['mean_fit_time']\n",
    "time_sds = grid.cv_results_['std_fit_time']\n",
    "\n",
    "results = []\n",
    "for (params, mean_score, score_sd, mean_fit_time, time_sd) in zip(params, mean_scores, score_sds, mean_fit_times, time_sds):\n",
    "    alpha = params['neural_net__alpha']\n",
    "    structure = params['neural_net__hidden_layer_sizes']\n",
    "    results.append([str(structure), alpha, mean_score, score_sd, mean_fit_time, time_sd])\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.columns = ['structure', 'c', 'mean_score', 'score_sd', 'mean_time', 'time_sd']\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the SBNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-23T23:42:34.225519Z",
     "start_time": "2020-09-23T23:42:34.155761Z"
    }
   },
   "outputs": [],
   "source": [
    "with open(classifiers_directory_path + '/sbnn.p', 'wb') as f:\n",
    "    pickle.dump(sbnn, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "p3normal",
   "language": "python",
   "name": "p3normal"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "247.067px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
